# -*- coding: utf-8 -*-
"""TextEmotionEstimationModel.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1bz3p3diH6KRsU-I4JcSWlcIDrpi0lS3x

# E5を事前学習済みモデルとした感情推定
"""

!pip install keras-core "jax[cuda]" jaxlib

import torch.nn.functional as F
from torch import Tensor
from transformers import AutoTokenizer, AutoModel
e5_tokenizer = AutoTokenizer.from_pretrained("Numind/e5-multilingual-sentiment_analysis")
e5_model = AutoModel.from_pretrained("Numind/e5-multilingual-sentiment_analysis")

# 平均プーリング
def average_pool(last_hidden_states: Tensor,
                attention_mask: Tensor) -> Tensor:
    last_hidden = last_hidden_states.masked_fill(~attention_mask[..., None].bool(), 0.0)
    return last_hidden.sum(dim=1) / attention_mask.sum(dim=1)[..., None]

def extE5( input_texts):
    # Tokenize the input texts
    batch_dict = e5_tokenizer(input_texts, max_length=512, padding=True, truncation=True, return_tensors='pt')
    outputs = e5_model(**batch_dict)
    embeddings = average_pool(outputs.last_hidden_state, batch_dict['attention_mask'])
    embeddings = F.normalize(embeddings, p=2, dim=1)
    scores = (embeddings[:2] @ embeddings[2:].T) * 100
    emb = embeddings.to('cpu').detach().numpy().copy()
    return emb

# モデルのロード
import gdown
gdown.download('https://drive.google.com/uc?id=1pfiC4-waJPUBjdcoF4hDLCHNTQB_OR_m', "model.keras.weights.h5")
gdown.download('https://drive.google.com/uc?id=1SR76rllIEMNDUZi6ro2HkNXon1FdRH96', "model.keras")

!mkdir model_E5senti_em8
!mv model.keras.weights.h5 model_E5senti_em8/model.keras.weights.h5
!mv model.keras model_E5senti_em8/model.keras

def toStr(vecs, emos):
    strings, labelList = [], []
    for vec in vecs:
        ss = ''
        labels = []
        for i, v in enumerate(vec):
            ss += f'{v:.5f}\t'
            if v >= 0.2:
                labels.append([emos[i], v])
        ss = ss.rstrip('\t')
        strings.append(ss)
        sxs = ''
        for l in labels:
            sxs += f'{l[0]}:{l[1]} '
        sxs = sxs.rstrip(' ')
        labelList.append(sxs)

import os, sys, re, glob
os.environ['KERAS_BACKEND'] = 'jax'
# JAXバックエンドでのメモリ断片化を回避
os.environ["XLA_PYTHON_CLIENT_MEM_FRACTION"]="1.00"
import numpy as np
from keras_core import datasets, layers, models
import keras_core as keras
from keras_core.callbacks import EarlyStopping
import pandas as pd

mdir = './model_E5senti_em8'
loaded_model = keras.saving.load_model(f"{mdir}/model.keras")
loaded_model.load_weights(f"{mdir}/model.keras.weights.h5")
loaded_model.summary()

emos = ['Joy', 'Sadness', 'Anticipation', 'Surprise', 'Anger', 'Fear', 'Disgust', 'Trust']


def toStr(vecs, emos):
    strings, labelList = [], []
    for vec in vecs:
        ss = ''
        labels = []
        for i, v in enumerate(vec):
            ss += f'{v:.5f}\t'
            if v >= 0.2:
                labels.append([emos[i], v])
        ss = ss.rstrip('\t')
        strings.append(ss)
        sxs = ''
        for l in labels:
            sxs += f'{l[0]}:{l[1]} '
        sxs = sxs.rstrip(' ')
        labelList.append(sxs)

    return strings, labelList


sentences = ['今日から長期休暇だぁーーー！！！', 'めっちゃ楽しみ♡']
vL = extE5(sentences)
vL = np.array(vL)

vL = np.reshape(vL, (len(vL), 768))
r = loaded_model.predict(vL)
rs, ls = toStr(r, emos)

for i, (l,r) in enumerate(zip(ls, rs)):
    print(l)
    print(f"[{sentences[i]}]\tResult: : {emos[np.argmax(r)]}")

"""# RosettaJA を用いた感情推定"""

# モデルのダウンロード
gdown.download("https://drive.google.com/uc?id=1cl5lOLOmmKSLGeMBlg-Xro2C5nyuKyK3", 'model.keras')
gdown.download("https://drive.google.com/uc?id=1YkKGaEbZUAAK4oG-x_adFHqzQ9szAPXW", 'model.keras.weights.h5')

!mkdir model_RosettaJA_em8
!mv model.keras.weights.h5 model_RosettaJA_em8/model.keras.weights.h5
!mv model.keras model_RosettaJA_em8/model.keras

# pkshatech/RoSEtta-base-ja(テキスト特徴量)用
from sentence_transformers import SentenceTransformer

rosetta_model = SentenceTransformer("pkshatech/RoSEtta-base-ja",trust_remote_code=True)

# Rosetta（テキストベクトル）の抽出
def extRosetta( input_texts):
    embeddings = rosetta_model.encode( input_texts, convert_to_tensor=True)
    return embeddings

m_rosedir = './model_RosettaJA_em8'
loaded_model = keras.saving.load_model(f"{m_rosedir}/model.keras")
loaded_model.load_weights(f"{m_rosedir}/model.keras.weights.h5")
loaded_model.summary()

emos = ['Joy', 'Sadness', 'Anticipation', 'Surprise', 'Anger', 'Fear', 'Disgust', 'Trust']

sentences = ['今日から長期休暇だぁーーー！！！', 'めっちゃ楽しみ♡']
vL = extRosetta(sentences)
vL = vL.to('cpu').detach().numpy().copy()

vL = np.reshape(vL, (len(vL), 768))
print(vL.shape)
res = loaded_model.predict(vL)
rs, ls = toStr(res, emos)

for i, (l,r) in enumerate(zip(ls, rs)):
    print(l)
    print(f"[{sentences[i]}]\tResult: : {emos[np.argmax(r)]}")